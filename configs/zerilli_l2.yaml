# Zerilli (even parity) l=2 experiment config in units where M=1.
# IMPROVED MODEL: Trainable RFF + Modified MLP (Wang et al. 2021, 2023)
experiment:
  name: zerilli_l2

physics:
  M: 1.0
  l: 2
  potential: zerilli   # zerilli | regge-wheeler
  pde_sign: standard   # standard: phi_tt - phi_xx + V phi = 0

domain:
  xmin: -50.0
  xmax: 150.0
  tmin: 0.0
  tmax: 50.0

initial_data:
  A: 1.0
  x0: 4.0
  sigma: 5.0
  velocity_profile: outgoing   # outgoing | paper

fd:
  dx: 0.2
  dt: 0.1
  scheme: rk4_mol

pinn:
  seed: 1234
  dtype: float64

  # Memory safeguard: compute residual terms in chunks to avoid very large autograd graphs.
  chunk_size: 4096   # set 0 for full-batch residual evaluation

  # training points
  Nr: 32000
  Ni: 800
  Nb: 400

  # network (paper: [80, 40, 20, 10])
  hidden_layers: [80, 40, 20, 10]
  activation: tanh
  output_transform: tanh_bound  # tanh_bound | none

  # Trainable Random Fourier Features (Wang, Yu & Perdikaris 2023)
  # Embeds raw (x*, t) into [cos(Bx), sin(Bx)] before the first hidden layer.
  # B is initialised from N(0, sigma) and is trainable so the network can
  # tune its frequencies to match the QNM oscillation (omega ~ 0.37).
  rff:
    num_features: 64    # output dim = 2*num_features = 128
    sigma: 1.0          # initial frequency spread
    trainable: true      # let the network tune the frequencies

  # loss weights λ = [Lr, Lrx, Lrt, Lic, Liv, Lbl, Lbr]
  # Paper's weights: λ_iv=100 to address phase problem (time-shift symmetry).
  lambda: [100.0, 100.0, 100.0, 1.0, 100.0, 1.0, 1.0]

  # Causal training (Wang et al., 2022 — arXiv:2203.07404)
  # Disabled for now. Was essentially inactive in previous run (w_min=0.9996).
  # Can be re-enabled after DeepXDE migration is validated.
  causal:
    enabled: false
    epsilon: 50.0    # causality strictness (0 = standard PINN, large = strict)
    n_slices: 20     # number of temporal bins

  # Gradient balancing (Wang, Teng & Perdikaris 2021)
  # Disabled: previous runs showed it deflates λ_iv, worsening the phase problem.
  # Using paper's fixed weights instead.
  gradient_balancing:
    enabled: false
    period: 100      # recompute weights every N Adam steps
    alpha: 0.9       # EMA decay (higher = slower adaptation)

  # optimizers
  adam:
    lr: 1.0e-3
    iters: 10000
    resample_period: 100

  lbfgs:
    iters: 15000
    resample_period: 100

evaluation:
  xq: 10.0   # sampling point for ringdown / QNM extraction

qnm:
  # time window used for fitting / FFT (to reduce initial transient)
  t_start: 10.0
  t_end: 50.0
